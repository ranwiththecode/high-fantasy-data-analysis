{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv9gxN7IHKP/YB0/C08k4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranwiththecode/high-fantasy-data-analysis/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biCTFYWm6E4q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Configuration (UPDATE THESE VALUES)\n",
        "CONFIG = {\n",
        "    \"input_file\": \"/content/drive/MyDrive/Goodreads_Data/tricksters_choice_clean.csv\",\n",
        "    \"common_keywords\": [\n",
        "        \"protagonist\", \"main character\", \"mc\", \"lead character\", \"main protagonist\"\n",
        "    ],\n",
        "    \"book_specific_keywords\": [\n",
        "        \"female main character\", \"female protagonist\", \"FMC\", \"hero\", \"heroine\", \"Aly\", \"Daughter of the Lionness\"\n",
        "    ],\n",
        "    \"timeout_seconds\": 300\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Normalize text for better matching\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"â€™\", \"'\")  # Handle curly apostrophes\n",
        "    text = text.replace(\"-\", \" \")  # Handle hyphenated words\n",
        "    return text\n",
        "\n",
        "def expand_keyword_variants(keywords):\n",
        "    \"\"\"Generate plural and possessive forms\"\"\"\n",
        "    expanded = []\n",
        "    for kw in keywords:\n",
        "        kw = kw.lower()\n",
        "        expanded.append(kw)\n",
        "        if not kw.endswith(\"s\"):\n",
        "            expanded.append(kw + \"s\")  # plural\n",
        "        if not kw.endswith(\"'s\"):\n",
        "            expanded.append(kw + \"'s\")  # possessive\n",
        "        if kw.endswith(\"s\") and not kw.endswith(\"s'\"):\n",
        "            expanded.append(kw + \"'\")  # plural possessive\n",
        "    return list(set(expanded))  # Remove duplicates\n",
        "\n",
        "def extract_keyword_context(text, keywords):\n",
        "    \"\"\"Robust keyword extraction with variant matching\"\"\"\n",
        "    text = preprocess_text(text)\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # Expand keywords to include variants\n",
        "    expanded_keywords = expand_keyword_variants(keywords)\n",
        "\n",
        "    # Create regex pattern that matches any variant\n",
        "    pattern = r'\\b(?:' + '|'.join(map(re.escape, expanded_keywords)) + r')\\b'\n",
        "\n",
        "    # Split sentences while preserving punctuation\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    # Find sentences containing any keyword variant\n",
        "    matches = [s for s in sentences if re.search(pattern, s)]\n",
        "    return matches\n",
        "\n",
        "def analyze_sentiment_for_category(df, keywords, rating_filter=None, desc=\"\"):\n",
        "    \"\"\"Analyze sentiment for a specific review subset\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Filter reviews\n",
        "    if rating_filter is not None:\n",
        "        subset = df[df['rating'].isna()] if pd.isna(rating_filter) else df[df['rating'] == rating_filter]\n",
        "    else:\n",
        "        subset = df\n",
        "\n",
        "    if subset.empty:\n",
        "        print(f\"No reviews found for {desc}\")\n",
        "        return None\n",
        "\n",
        "    # Extract keyword sentences with progress tracking\n",
        "    keyword_sentences = []\n",
        "    progress_bar = tqdm(subset['text'], desc=f\"Processing {desc}\")\n",
        "\n",
        "    for text in progress_bar:\n",
        "        if time.time() - start_time > CONFIG['timeout_seconds']:\n",
        "            print(f\"\\nTimeout reached for {desc}\")\n",
        "            break\n",
        "\n",
        "        matches = extract_keyword_context(text, keywords)\n",
        "        keyword_sentences.extend(matches)\n",
        "        progress_bar.set_postfix({'matches': len(keyword_sentences)})\n",
        "\n",
        "    # Debug output\n",
        "    if not keyword_sentences:\n",
        "        print(\"\\nDebug: No matches found. Sample reviews:\")\n",
        "        for i, text in enumerate(subset['text'].sample(min(3, len(subset))), 1):\n",
        "            print(f\"\\nSample {i}:\")\n",
        "            print(text[:200] + (\"...\" if len(text) > 200 else \"\"))\n",
        "        return None\n",
        "\n",
        "    # Calculate combined sentiment\n",
        "    combined_text = ' '.join(keyword_sentences)\n",
        "    try:\n",
        "        sentiment = sid.polarity_scores(combined_text)['compound']\n",
        "        print(f\"\\nFound {len(keyword_sentences)} matches for {desc}\")\n",
        "        return round(sentiment, 4)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nSentiment calculation failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def analyze_book_sentiment():\n",
        "    \"\"\"Main analysis function\"\"\"\n",
        "    # Load and validate data\n",
        "    try:\n",
        "        print(f\"\\nLoading data from: {CONFIG['input_file']}\")\n",
        "        df = pd.read_csv(CONFIG['input_file'])\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"Error: Empty DataFrame\")\n",
        "            return None\n",
        "\n",
        "        # Check required columns\n",
        "        for col in ['text', 'rating']:\n",
        "            if col not in df.columns:\n",
        "                print(f\"Error: Missing column '{col}'\")\n",
        "                return None\n",
        "\n",
        "        # Clean ratings\n",
        "        df['rating'] = df['rating'].replace(['NULL', 'null', 'NA', ''], pd.NA)\n",
        "        print(f\"Loaded {len(df)} reviews ({df['rating'].isna().sum()} without ratings)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFailed to load data: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    # Run all analyses\n",
        "    results = {}\n",
        "    analyses = [\n",
        "        ('all_reviews', None, \"All reviews\"),\n",
        "        ('null_rated', pd.NA, \"NULL-rated reviews\"),\n",
        "        ('common_keywords', None, \"Common keywords\"),\n",
        "        ('book_keywords', None, \"Book keywords\")\n",
        "    ]\n",
        "\n",
        "    for key, rating_filter, desc in analyses:\n",
        "        print(f\"\\n=== Analyzing {desc} ===\")\n",
        "        keywords = (\n",
        "            CONFIG['common_keywords'] if \"common\" in desc.lower()\n",
        "            else CONFIG['book_specific_keywords'] if \"book\" in desc.lower()\n",
        "            else CONFIG['common_keywords'] + CONFIG['book_specific_keywords']\n",
        "        )\n",
        "\n",
        "        results[key] = analyze_sentiment_for_category(\n",
        "            df, keywords, rating_filter, desc\n",
        "        )\n",
        "        print(f\"Result: {results[key] if results[key] is not None else 'N/A'}\")\n",
        "\n",
        "    # Save and display results\n",
        "    results['book_title'] = os.path.splitext(os.path.basename(CONFIG['input_file']))[0]\n",
        "    results_df = pd.DataFrame([results])\n",
        "\n",
        "    output_path = os.path.splitext(CONFIG['input_file'])[0] + '_sentiment_results.csv'\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(\"\\n=== FINAL RESULTS ===\")\n",
        "    print(results_df.transpose())\n",
        "    print(f\"\\nResults saved to: {output_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    final_results = analyze_book_sentiment()"
      ]
    }
  ]
}