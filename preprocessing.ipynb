{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpUacpBtMgomecf2x58sHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranwiththecode/high-fantasy-data-analysis/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgoActJc4Dsu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Comprehensive emoji dictionary\n",
        "EMOJI_DICT = {\n",
        "    # Smileys & Emotion\n",
        "    \"ğŸ˜€\": \"[grinning_face]\", \"ğŸ˜ƒ\": \"[grinning_face_with_big_eyes]\", \"ğŸ˜„\": \"[grinning_face_with_smiling_eyes]\",\n",
        "    \"ğŸ˜\": \"[beaming_face_with_smiling_eyes]\", \"ğŸ˜†\": \"[grinning_squinting_face]\", \"ğŸ˜…\": \"[grinning_face_with_sweat]\",\n",
        "    \"ğŸ˜‚\": \"[face_with_tears_of_joy]\", \"ğŸ¤£\": \"[rolling_on_the_floor_laughing]\", \"ğŸ˜Š\": \"[smiling_face_with_smiling_eyes]\",\n",
        "    \"ğŸ˜‡\": \"[smiling_face_with_halo]\", \"ğŸ™‚\": \"[slightly_smiling_face]\", \"ğŸ™ƒ\": \"[upside_down_face]\",\n",
        "    \"ğŸ˜‰\": \"[winking_face]\", \"ğŸ˜Œ\": \"[relieved_face]\", \"ğŸ˜\": \"[smiling_face_with_heart_eyes]\",\n",
        "    \"ğŸ¥°\": \"[smiling_face_with_hearts]\", \"ğŸ˜˜\": \"[face_blowing_a_kiss]\", \"ğŸ˜—\": \"[kissing_face]\",\n",
        "    \"ğŸ˜™\": \"[kissing_face_with_smiling_eyes]\", \"ğŸ˜š\": \"[kissing_face_with_closed_eyes]\", \"ğŸ˜‹\": \"[face_savoring_food]\",\n",
        "    \"ğŸ˜›\": \"[face_with_tongue]\", \"ğŸ˜\": \"[squinting_face_with_tongue]\", \"ğŸ˜œ\": \"[winking_face_with_tongue]\",\n",
        "    \"ğŸ¤ª\": \"[zany_face]\", \"ğŸ¤¨\": \"[face_with_raised_eyebrow]\", \"ğŸ§\": \"[face_with_monocle]\",\n",
        "    \"ğŸ¤“\": \"[nerd_face]\", \"ğŸ˜\": \"[smiling_face_with_sunglasses]\", \"ğŸ¤©\": \"[star_struck]\",\n",
        "    \"ğŸ¥³\": \"[partying_face]\", \"ğŸ˜\": \"[smirking_face]\", \"ğŸ˜’\": \"[unamused_face]\",\n",
        "    \"ğŸ˜\": \"[disappointed_face]\", \"ğŸ˜”\": \"[pensive_face]\", \"ğŸ˜Ÿ\": \"[worried_face]\",\n",
        "    \"ğŸ˜•\": \"[confused_face]\", \"ğŸ™\": \"[slightly_frowning_face]\", \"â˜¹ï¸\": \"[frowning_face]\",\n",
        "    \"ğŸ˜£\": \"[persevering_face]\", \"ğŸ˜–\": \"[confounded_face]\", \"ğŸ˜«\": \"[tired_face]\",\n",
        "    \"ğŸ˜©\": \"[weary_face]\", \"ğŸ¥º\": \"[pleading_face]\", \"ğŸ˜¢\": \"[crying_face]\",\n",
        "    \"ğŸ˜­\": \"[loudly_crying_face]\", \"ğŸ˜¤\": \"[face_with_steam_from_nose]\", \"ğŸ˜ \": \"[angry_face]\",\n",
        "    \"ğŸ˜¡\": \"[pouting_face]\", \"ğŸ¤¬\": \"[face_with_symbols_on_mouth]\", \"ğŸ¤¯\": \"[exploding_head]\",\n",
        "    \"ğŸ˜³\": \"[flushed_face]\", \"ğŸ¥µ\": \"[hot_face]\", \"ğŸ¥¶\": \"[cold_face]\",\n",
        "    \"ğŸ˜±\": \"[face_screaming_in_fear]\", \"ğŸ˜¨\": \"[fearful_face]\", \"ğŸ˜°\": \"[anxious_face_with_sweat]\",\n",
        "    \"ğŸ˜¥\": \"[sad_but_relieved_face]\", \"ğŸ˜“\": \"[downcast_face_with_sweat]\", \"ğŸ¤—\": \"[hugging_face]\",\n",
        "    \"ğŸ¤”\": \"[thinking_face]\", \"ğŸ¤­\": \"[face_with_hand_over_mouth]\", \"ğŸ¤«\": \"[shushing_face]\",\n",
        "    \"ğŸ¤¥\": \"[lying_face]\", \"ğŸ˜¶\": \"[face_without_mouth]\", \"ğŸ˜\": \"[neutral_face]\",\n",
        "    \"ğŸ˜‘\": \"[expressionless_face]\", \"ğŸ˜¬\": \"[grimacing_face]\", \"ğŸ™„\": \"[face_with_rolling_eyes]\",\n",
        "    \"ğŸ˜¯\": \"[hushed_face]\", \"ğŸ˜¦\": \"[frowning_face_with_open_mouth]\", \"ğŸ˜§\": \"[anguished_face]\",\n",
        "    \"ğŸ˜®\": \"[face_with_open_mouth]\", \"ğŸ˜²\": \"[astonished_face]\", \"ğŸ¥±\": \"[yawning_face]\",\n",
        "    \"ğŸ˜´\": \"[sleeping_face]\", \"ğŸ¤¤\": \"[drooling_face]\", \"ğŸ˜ª\": \"[sleepy_face]\",\n",
        "    \"ğŸ˜µ\": \"[dizzy_face]\", \"ğŸ¤\": \"[zipper_mouth_face]\", \"ğŸ¥´\": \"[woozy_face]\",\n",
        "    \"ğŸ¤¢\": \"[nauseated_face]\", \"ğŸ¤®\": \"[face_vomiting]\", \"ğŸ¤§\": \"[sneezing_face]\",\n",
        "    \"ğŸ˜·\": \"[face_with_medical_mask]\", \"ğŸ¤’\": \"[face_with_thermometer]\", \"ğŸ¤•\": \"[face_with_head_bandage]\",\n",
        "\n",
        "    # Hearts\n",
        "    \"â¤ï¸\": \"[red_heart]\", \"ğŸ§¡\": \"[orange_heart]\", \"ğŸ’›\": \"[yellow_heart]\",\n",
        "    \"ğŸ’š\": \"[green_heart]\", \"ğŸ’™\": \"[blue_heart]\", \"ğŸ’œ\": \"[purple_heart]\",\n",
        "    \"ğŸ–¤\": \"[black_heart]\", \"ğŸ¤\": \"[white_heart]\", \"ğŸ¤\": \"[brown_heart]\",\n",
        "    \"ğŸ’”\": \"[broken_heart]\", \"â¤ï¸â€ğŸ”¥\": \"[heart_on_fire]\", \"â¤ï¸â€ğŸ©¹\": \"[mending_heart]\",\n",
        "    \"ğŸ’•\": \"[two_hearts]\", \"ğŸ’\": \"[revolving_hearts]\", \"ğŸ’“\": \"[beating_heart]\",\n",
        "    \"ğŸ’—\": \"[growing_heart]\", \"ğŸ’–\": \"[sparkling_heart]\", \"ğŸ’˜\": \"[heart_with_arrow]\",\n",
        "    \"ğŸ’\": \"[heart_with_ribbon]\", \"ğŸ’Ÿ\": \"[heart_decoration]\",\n",
        "\n",
        "    # Common symbols\n",
        "    \"â­\": \"[star]\", \"ğŸŒŸ\": \"[glowing_star]\", \"âœ¨\": \"[sparkles]\",\n",
        "    \"ğŸ’«\": \"[dizzy]\", \"ğŸ”¥\": \"[fire]\", \"ğŸ’¯\": \"[hundred_points]\",\n",
        "    \"ğŸ’¢\": \"[anger_symbol]\", \"ğŸ’¥\": \"[collision]\", \"ğŸ’¦\": \"[sweat_droplets]\",\n",
        "    \"ğŸ’¨\": \"[dashing_away]\", \"ğŸ’£\": \"[bomb]\", \"ğŸ’¬\": \"[speech_balloon]\",\n",
        "    \"ğŸ‘ï¸â€ğŸ—¨ï¸\": \"[eye_in_speech_bubble]\", \"ğŸ—¨ï¸\": \"[left_speech_bubble]\", \"ğŸ—¯ï¸\": \"[right_anger_bubble]\",\n",
        "    \"ğŸ’­\": \"[thought_balloon]\", \"ğŸ’¤\": \"[zzz]\",\n",
        "\n",
        "    # Books & writing\n",
        "    \"ğŸ“š\": \"[books]\", \"ğŸ“–\": \"[open_book]\", \"ğŸ“•\": \"[closed_book]\",\n",
        "    \"ğŸ“˜\": \"[blue_book]\", \"ğŸ“™\": \"[orange_book]\", \"ğŸ“—\": \"[green_book]\",\n",
        "    \"ğŸ““\": \"[notebook]\", \"ğŸ“”\": \"[notebook_with_decorative_cover]\", \"ğŸ“’\": \"[ledger]\",\n",
        "    \"ğŸ“‘\": \"[bookmark_tabs]\", \"ğŸ“\": \"[memo]\", \"ğŸ’»\": \"[laptop]\",\n",
        "\n",
        "    # Hands\n",
        "    \"ğŸ‘\": \"[thumbs_up]\", \"ğŸ‘\": \"[thumbs_down]\", \"âœŠ\": \"[raised_fist]\",\n",
        "    \"ğŸ‘Š\": \"[oncoming_fist]\", \"ğŸ¤›\": \"[left_facing_fist]\", \"ğŸ¤œ\": \"[right_facing_fist]\",\n",
        "    \"ğŸ‘\": \"[clapping_hands]\", \"ğŸ™Œ\": \"[raising_hands]\", \"ğŸ‘\": \"[open_hands]\",\n",
        "    \"ğŸ¤²\": \"[palms_up_together]\", \"ğŸ¤\": \"[handshake]\", \"ğŸ™\": \"[folded_hands]\",\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean text by removing HTML tags, normalizing emojis, and standardizing whitespace\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove HTML tags\n",
        "    clean_text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Convert emojis to text descriptions\n",
        "    for emoji, desc in EMOJI_DICT.items():\n",
        "        clean_text = clean_text.replace(emoji, f\" {desc} \")\n",
        "\n",
        "    # Standardize whitespace and clean up\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "def is_english(text, min_english_chars=0.7):\n",
        "    \"\"\"Simple English detection using character analysis\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return False\n",
        "\n",
        "    # Count ASCII letters and basic punctuation\n",
        "    english_chars = sum(1 for c in text if ord(c) < 128 or c in 'â€˜â€™â€œâ€â€“â€”')\n",
        "    total_chars = max(1, len(text))  # avoid division by zero\n",
        "\n",
        "    return (english_chars / total_chars) >= min_english_chars\n",
        "\n",
        "def preprocess_reviews_file(input_path, output_path):\n",
        "    \"\"\"Process the entire CSV file with progress tracking and validation\"\"\"\n",
        "    try:\n",
        "        # Read with error handling\n",
        "        df = pd.read_csv(input_path, keep_default_na=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Validate input structure\n",
        "    if 'text' not in df.columns or 'rating' not in df.columns:\n",
        "        print(\"Error: Input file must contain 'text' and 'rating' columns\")\n",
        "        return False\n",
        "\n",
        "    # Convert 0 ratings to NULL (Goodreads uses 0 for \"no rating\")\n",
        "    df['rating'] = df['rating'].replace(0, pd.NA)\n",
        "\n",
        "    # Initialize progress bar for preprocessing\n",
        "    tqdm.pandas(desc=\"Cleaning text\")\n",
        "    df['clean_text'] = df['text'].progress_apply(preprocess_text)\n",
        "\n",
        "    # Filter English reviews with progress\n",
        "    tqdm.pandas(desc=\"Checking language\")\n",
        "    df['is_english'] = df['clean_text'].progress_apply(is_english)\n",
        "    english_df = df[df['is_english']].copy()\n",
        "\n",
        "    # Keep only needed columns\n",
        "    english_df = english_df[['rating', 'clean_text']]\n",
        "    english_df.columns = ['rating', 'text']  # Standardize column names\n",
        "\n",
        "    # Save with error handling\n",
        "    try:\n",
        "        english_df.to_csv(output_path, index=False, encoding='utf-8', na_rep='NULL')\n",
        "        print(f\"\\nSuccess! Cleaned {len(english_df)}/{len(df)} reviews saved to: {output_path}\")\n",
        "        print(f\"Reviews with valid ratings: {english_df['rating'].notna().sum()}\")\n",
        "        print(f\"Reviews without ratings: {english_df['rating'].isna().sum()}\")\n",
        "\n",
        "        # Show sample of cleaned data\n",
        "        print(\"\\nSample of cleaned reviews:\")\n",
        "        print(english_df.sample(min(5, len(english_df))).to_string(index=False))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "        return False\n",
        "\n",
        "
        "if __name__ == \"__main__\":\n",
        "    # Set your file paths\n",
        "    BOOK_TITLE = \"eragon_paolini\"  # Change book title\n",
        "    INPUT_PATH = f'/content/drive/MyDrive/Goodreads_Data/{BOOK_TITLE}_reviews.csv'\n",
        "    OUTPUT_PATH = f'/content/drive/MyDrive/Goodreads_Data/{BOOK_TITLE}_clean.csv'\n",
        "\n",
        "    # Run preprocessing\n",
        "    print(f\"Starting preprocessing for {BOOK_TITLE}...\")\n",
        "    success = preprocess_reviews_file(INPUT_PATH, OUTPUT_PATH)"
      ]
    }
  ]
}
